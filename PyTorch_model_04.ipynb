{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Saloni0512/PyTorch-practice-/blob/main/PyTorch_model_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pninZjaDYrjC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjsUqcy-Cote"
   },
   "source": [
    "# PyTorch custom datasets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "B86ruLrOZPim",
    "outputId": "46e63553-5e07-49ac-e28d-d8b43dec11c6"
   },
   "outputs": [],
   "source": [
    "# Setting up device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsbVZjQLEAHx"
   },
   "source": [
    "## 1. Get data\n",
    "here the dataset is a subset of the Food101 dataset.\n",
    "\n",
    "It contains only 10% of the images (~75 training and ~25 testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7hinANfEEZt",
    "outputId": "1f823651-1e74-4508-c1da-aff28aefcadb"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to a data folder\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# If the image folder doesnt exist, download and prepare it\n",
    "if image_path.is_dir():\n",
    "  print(f\"{image_path} directory already exists\")\n",
    "else:\n",
    "  print(f\"{image_path} directory doesnt exist... creating one\")\n",
    "  image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the data\n",
    "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "  f.write(request.content)\n",
    "\n",
    "# Unzip the data\n",
    "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "  zip_ref.extractall(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h9s9Q9ObaRt"
   },
   "source": [
    "## 2. Data prep and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tFzbAvJLOsz",
    "outputId": "be084f94-f8d9-40ad-c04c-75f8216608e2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"Displays the brief overview of a given directory.\"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"there are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'\")\n",
    "\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRMdRzXheBN-",
    "outputId": "6da615c8-8fba-4f23-d5c6-be7f71ee0b90"
   },
   "outputs": [],
   "source": [
    "# Setting up training and testing paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzNV1VZXfPyF"
   },
   "source": [
    "### 2.1 Visualising the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "sq2SejgzeqpS",
    "outputId": "a861d5bf-bc6a-49b6-b728-a737639db641"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Set the seed\n",
    "random.seed(42)\n",
    "\n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "# image class is the name of the directory in which image is stored\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "# open image\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# print metadata\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"image class: {image_class}\")\n",
    "print(f\"image height: {img.height}\")\n",
    "print(f\"image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "QHqoatwzxpDg",
    "outputId": "4ecdf3a9-dce1-4ed9-b70e-9e8433c1fca4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height,width,color_channels]\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28gJb9jq6qaC",
    "outputId": "f6c3cfb2-2314-46b6-ce23-73b490d16746"
   },
   "outputs": [],
   "source": [
    "print(img_as_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wRC8L36KO0M"
   },
   "source": [
    "## 3. Transforming data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pNL-W_v0E0J"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ly0RxiKobyrJ"
   },
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor() # turns the image into torch.Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDMzntvlcwb8",
    "outputId": "e51b9168-da18-4e96-9001-8d96f9f37d50"
   },
   "outputs": [],
   "source": [
    "transform_data(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyE8g6krSryk",
    "outputId": "8309f59c-68a4-4855-8b80-7c1d12f95c91"
   },
   "outputs": [],
   "source": [
    "transform_data(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Wm5mHrPzS2-D",
    "outputId": "e43e9264-51cc-43b0-d5d5-487bb64a7a21"
   },
   "outputs": [],
   "source": [
    "# Let's visualise the transformed images\n",
    "def plot_transformed_image(image_paths: list, transform, n=3, seed=None):\n",
    "  if seed:\n",
    "    random.seed(seed)\n",
    "  random_image_paths = random.sample(image_paths, k=n)\n",
    "  for image_path in random_image_paths:\n",
    "    with Image.open(image_path) as f:\n",
    "      fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "      ax[0].imshow(f)\n",
    "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
    "      ax[0].axis(False)\n",
    "\n",
    "      transformed_image = transform(f).permute(1,2,0) # matplotlib expects the HWC format, so we use permute here\n",
    "      ax[1].imshow(transformed_image)\n",
    "      ax[1].set_title(f\"Transformed\\nSize: {transformed_image.shape}\")\n",
    "      ax[1].axis(\"off\")\n",
    "\n",
    "      fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=14)\n",
    "\n",
    "plot_transformed_image(image_path_list,\n",
    "                       transform=transform_data,\n",
    "                       n=3,\n",
    "                       seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMCdZW3i3pxS"
   },
   "source": [
    "## 4. Option1: Loading image data using `ImageFolder`\n",
    "ImageFolder is a prebuilt datasets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCadVyCZul4f",
    "outputId": "f7e70b02-34d3-4703-b623-67cfbeea1d9e"
   },
   "outputs": [],
   "source": [
    "# Use ImageFolder to create dataset(s)\n",
    "train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                   transform=transform_data,\n",
    "                                   target_transform=None)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                 transform=transform_data,\n",
    "                                 target_transform=None)\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeYuF51JR31J",
    "outputId": "8454a123-e02a-483b-dab0-010ebab081cf"
   },
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLNO6-QadR1q",
    "outputId": "1dba7353-5ca9-44f6-b714-265052c93d1b"
   },
   "outputs": [],
   "source": [
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeOUP5eOdhNn",
    "outputId": "cc64f8ba-4457-402e-e55a-c861879867b6"
   },
   "outputs": [],
   "source": [
    "train_data.samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBrz5Qi7eVqU",
    "outputId": "85bf1ee2-4b31-46d5-b4c7-3996feb2b37d"
   },
   "outputs": [],
   "source": [
    "# Index on the train data to get a single image and its label\n",
    "img, label = train_data[0][0] , train_data[0][1]\n",
    "print(f\"Image tensor: \\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Image class: {class_names[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCyOj6fggfsi"
   },
   "source": [
    "### 4.1 Turn loaded image data into `DataLoader`\n",
    "A dataloader helps us to turn Dataset into iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4AxVp_texLw",
    "outputId": "e846c775-ea4a-4398-f45f-9f566924ae78"
   },
   "outputs": [],
   "source": [
    "# Lets turn train data and test data into DataLoader\n",
    "BATCH_SIZE = 1 # hyperparam\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=1, # no of CPU cores used to load the data , use os.cpu_count() as value if more cpus needed\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=1,\n",
    "                             shuffle=False)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STlLyOwmjxZ6",
    "outputId": "a1719cc6-b95c-4957-edff-a2aec8c2217a"
   },
   "outputs": [],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqK3j7Q_kN9K",
    "outputId": "7d6be566-6ebc-41d7-dad6-6af9f7dcf477"
   },
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja5Kn0U8lja0"
   },
   "source": [
    "## 5. Option 2: Loading image data using a Custom `Dataset`\n",
    "* load images from a file\n",
    "* get class names from the dataset\n",
    "* get classes as dictionary from the dataset\n",
    ">Note: We can create a `Dataset` almost out of anything, but it also means writing more code that could have errors or lead to performance issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCucLNJ1ljIq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAIU4nJXg9qE"
   },
   "source": [
    "### 5.1 Get class names from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ3DTNb1m78v",
    "outputId": "2b7cc60c-6acf-4ae5-a977-6bd79f08ceca"
   },
   "outputs": [],
   "source": [
    "# create a target directory\n",
    "target_dir = train_dir\n",
    "\n",
    "# Get class names from the target directory\n",
    "class_names_found = sorted([entry.name for entry in list(os.scandir(target_dir))])\n",
    "class_names_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnFgEh0CfZwk",
    "outputId": "760d0321-ab27-41bc-d6f2-7d5650e75a70"
   },
   "outputs": [],
   "source": [
    "list(os.scandir(target_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zid0muT0fdSA"
   },
   "outputs": [],
   "source": [
    "# A helper function that we will use in our custom dataset\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "  \"\"\"Find class folder names in the target directory\"\"\"\n",
    "  classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "  classes_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "\n",
    "  return classes, classes_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKtlVGhqg13M",
    "outputId": "84d272ac-bd8f-4430-a394-de32d88b3419"
   },
   "outputs": [],
   "source": [
    "find_classes(target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qXy2vJelQU0"
   },
   "source": [
    "### 5.2 Create a Custom `Dataset` to replicate `ImageFolder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyMiEpIlkhcb"
   },
   "outputs": [],
   "source": [
    "# Write a custom dataset class\n",
    "import pathlib\n",
    "class ImageFolderCustom(Dataset):\n",
    "  # Initialise the custom dataset\n",
    "  def __init__(self,\n",
    "               targ_dir: str,\n",
    "               transform=None):\n",
    "    self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\"))\n",
    "    self.transform = transform\n",
    "    self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "  def load_image(self, index: int) -> Image.Image:\n",
    "    \"\"\"Opens an image via path and returns it.\"\"\"\n",
    "    image_path = self.paths[index]\n",
    "    return Image.open(image_path)\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    \"\"\"Returns the total number of samples.\"\"\"\n",
    "    return len(self.paths)\n",
    "\n",
    "  def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"Returns one sample of data, data and label (X, y).\"\"\"\n",
    "    img = self.load_image(index)\n",
    "    class_name = self.paths[index].parent.name\n",
    "    class_to_idx = self.class_to_idx[class_name]\n",
    "\n",
    "    if self.transform:\n",
    "      return self.transform(img), class_to_idx # transformed img and label\n",
    "    else:\n",
    "      return img, class_to_idx # untransformed img and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzy5WXl1BOgs"
   },
   "outputs": [],
   "source": [
    "# Create a transform\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # data augmentation technique used to increase diversity of training data\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dPnUw-gCqux",
    "outputId": "8f1f064d-5510-4e47-9fd9-0aa25f4f7699"
   },
   "outputs": [],
   "source": [
    "# Test the ImageFolderCustom\n",
    "train_data_custom = ImageFolderCustom(targ_dir=train_dir,\n",
    "                                      transform=train_transforms)\n",
    "\n",
    "test_data_custom = ImageFolderCustom(targ_dir=test_dir,\n",
    "                                    transform=test_transforms)\n",
    "train_data_custom, test_data_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kY6FtlDRHq22",
    "outputId": "8a3e5fd8-92f5-49bc-a2f4-b451e6865e35"
   },
   "outputs": [],
   "source": [
    "len(train_data_custom), len(test_data_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvG7YQZvHzjG",
    "outputId": "e9a8f5a6-e265-4866-918c-3ef48a3f648e"
   },
   "outputs": [],
   "source": [
    "train_data_custom.classes , test_data_custom.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcXgUDZaIIh2",
    "outputId": "ae84bc83-563e-40a2-8afa-25fb664b4dd5"
   },
   "outputs": [],
   "source": [
    "train_data_custom.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcL4d2esH8WB",
    "outputId": "37c35b6b-3a4a-4f23-ba13-f871c88db7e9"
   },
   "outputs": [],
   "source": [
    "train_data_custom.load_image(1), test_data_custom.load_image(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpCJNEN2I-oL",
    "outputId": "cb0a94e4-4f2d-4200-810b-e528333e63ac"
   },
   "outputs": [],
   "source": [
    "train_data_custom.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYLchaaEJgDl"
   },
   "source": [
    "### 5.3 Create a function to display random images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CslsW_wtKK_N"
   },
   "outputs": [],
   "source": [
    "def display_random_images(dataset: torch.utils.data.Dataset,\n",
    "                          classes: List[str]= None,\n",
    "                          n : int = 10, # no of images to display\n",
    "                          display_shape: bool = True,\n",
    "                          seed: int = None):\n",
    "  if n > 10:\n",
    "    n= 10\n",
    "    display_shape = False\n",
    "    print(f\"For display purposes, n shouldn't be larger than 10, setting n = 10 and display_shape = False\")\n",
    "\n",
    "  if seed:\n",
    "    random.seed(seed)\n",
    "\n",
    "  random_sample_idxs = random.sample(range(len(train_data_custom)), k=n)\n",
    "  plt.figure(figsize=(16,8))\n",
    "\n",
    "  for i, targ_sample in enumerate(random_sample_idxs):\n",
    "    targ_img, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
    "    targ_img_adjust = targ_img.permute(1,2,0) # matplotlib works with HWC order\n",
    "    # Plot adjusted samples\n",
    "    plt.subplot(1,n,i+1)\n",
    "    plt.imshow(targ_img_adjust)\n",
    "    if classes:\n",
    "      title = f\"class: {classes[targ_label]}\"\n",
    "      if display_shape:\n",
    "        title = title + f\"\\nshape: {targ_img_adjust.shape}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "TcZ28ESffmVo",
    "outputId": "e74ad4a6-cb2c-4d84-9edf-627fed7278b3"
   },
   "outputs": [],
   "source": [
    "# Show random images from ImageFolderCustom dataset\n",
    "display_random_images(train_data_custom,\n",
    "                      class_names,\n",
    "                      n=15,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIe0_R4DhxUL"
   },
   "source": [
    "### 5.4 Turn image data from custom dataset into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXxHvukUhx1a",
    "outputId": "7aa3981c-3dbb-4bb9-c3f5-c686f580a056"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1 # hyperparam\n",
    "train_custom_dataloader = DataLoader(dataset=train_data_custom,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=1, # no of CPU cores used to load the data , use os.cpu_count() as value if more cpus needed\n",
    "                              shuffle=True)\n",
    "\n",
    "test_custom_dataloader = DataLoader(dataset=test_data_custom,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=1,\n",
    "                             shuffle=False)\n",
    "\n",
    "train_custom_dataloader, test_custom_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ROT_eyQi7g7",
    "outputId": "9bdf9922-8374-4e85-85be-d899f48cb077"
   },
   "outputs": [],
   "source": [
    "img, label = next(iter(train_custom_dataloader))\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLi0DzppkKnZ"
   },
   "source": [
    "## 6. Experimenting with data augmentation (other forms of transforms)\n",
    "Data augmentation is the process of artifically adding diversity to training data. This helps us to view images from different angles and perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCrXVHTOkSJE"
   },
   "outputs": [],
   "source": [
    "# 1. TrivialAugment\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(128,128)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(128,128)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F4vm7d5rnLYE",
    "outputId": "e7601112-0de4-46b5-eef7-f1f4fddb53a7"
   },
   "outputs": [],
   "source": [
    "plot_transformed_image(image_path_list,\n",
    "                       transform=train_transforms,\n",
    "                       n=3,\n",
    "                       seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S87cJCvgqnZR"
   },
   "source": [
    "## 7. Model 0: TinyVGG without data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRJoQDgHrs7G"
   },
   "source": [
    "### 7.1 Load data and create transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96xwh-nOsfRt"
   },
   "outputs": [],
   "source": [
    "# Create transform\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uvmz5FW_qm4G",
    "outputId": "93f37b9e-f37f-4b99-e8bf-048fef6d6343"
   },
   "outputs": [],
   "source": [
    "# Load and transform data\n",
    "train_data_simple = datasets.ImageFolder(root=train_dir,\n",
    "                                         transform=simple_transform,\n",
    "                                         target_transform=None)\n",
    "\n",
    "test_data_simple = datasets.ImageFolder(root=train_dir,\n",
    "                                         transform=simple_transform,\n",
    "                                         target_transform=None)\n",
    "\n",
    "# Turn data into dataloader\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataloader_simple = DataLoader(dataset=train_data_simple,\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     num_workers=NUM_WORKERS,\n",
    "                                     shuffle=True)\n",
    "\n",
    "test_dataloader_simple = DataLoader(dataset=test_data_simple,\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     num_workers=NUM_WORKERS,\n",
    "                                     shuffle=False)\n",
    "\n",
    "train_dataloader_simple, test_dataloader_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5BBiy3fTpw-",
    "outputId": "974997d1-fe67-4eac-c6d7-a8acb645f8ed"
   },
   "outputs": [],
   "source": [
    "len(train_dataloader_simple), len(test_dataloader_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYsfOxzNUUlz",
    "outputId": "06221302-4bd0-4559-8fca-ea418a0d3c0c"
   },
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader_simple))\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb81jDPoUcg2"
   },
   "source": [
    "### 7.2 Create TinyVGG model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9Cr9rbhUq54"
   },
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "  def __init__(self,\n",
    "               input_shape: int,\n",
    "               hidden_units: int,\n",
    "               output_shape: int) -> None:\n",
    "    super().__init__()\n",
    "    self.conv_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_shape,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2,\n",
    "                     stride=2)\n",
    "    )\n",
    "    self.conv_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2,\n",
    "                     stride=2)\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=hidden_units*13*13,\n",
    "                  out_features=output_shape)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.conv_block_1(x)\n",
    "    #print(x.shape)\n",
    "    x = self.conv_block_2(x)\n",
    "    #print(x.shape)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "    #return self.classifier(self.conv_block_2(self.conv_block_1(x))) # benefits from operator fusion - speeds up computation on GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQVyPwkoZv1Y",
    "outputId": "15f81d4d-5554-4ac8-93f0-6a698bce6e78"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_0 = TinyVGG(input_shape=3, # no of color channels in the image data\n",
    "                  hidden_units=10,\n",
    "                  output_shape=len(class_names)).to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1F3AxH_ntHB",
    "outputId": "1f110cec-e0d4-4c0c-b1e8-017512af8292"
   },
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ-NtiPnktVY"
   },
   "source": [
    "### 7.3 Trying forward pass on a single image to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0assRFYelAxv"
   },
   "outputs": [],
   "source": [
    "# Create a image batch and a label batch\n",
    "image_batch, label_batch = next(iter(train_dataloader_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "krvC3FFslak5",
    "outputId": "398e0059-c34b-4d48-fd69-2ac24c43897e"
   },
   "outputs": [],
   "source": [
    "# do the forward pass\n",
    "model_0(image_batch.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egIW3Sl5oSt9"
   },
   "source": [
    "### 7.4 Print model summary using `torchinfo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2RVhgfgnrut",
    "outputId": "16a24914-145a-41cc-aa3e-3aa216bacf33"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_GngeUgo-j0",
    "outputId": "fa6b9281-399b-4f58-88dd-cde536122c05"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "model_summary = summary(model_0, input_size=[1,3,64,64])\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqvJb-Fgqm1k"
   },
   "source": [
    "### 7.5 Creating training and testing loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSO7moC9quXp"
   },
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "  # Put the model in train mode\n",
    "  model.train()\n",
    "  train_loss, train_acc = 0,0\n",
    "\n",
    "  # loop through dataloader\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # send data to the target device\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    y_pred = model(X) # output raw logits\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate the accuracy metric\n",
    "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "    train_acc += ((y_pred_class == y).sum().item()/len(y_pred))\n",
    "\n",
    "  # Return the adjusted loss and accuracy metrics\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5HlRX6quh2b"
   },
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "  # put model in eval mode\n",
    "  model.eval()\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      test_pred = model(X)\n",
    "      loss = loss_fn(test_pred, y)\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      # Calculate the accuarcy metric\n",
    "      test_pred_label = test_pred.argmax(dim=1)\n",
    "      test_acc += ((test_pred_label == y).sum().item()/len(test_pred))\n",
    "\n",
    "  # Return the adjusted loss and accuracy metrics\n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_plebDDxuRn"
   },
   "source": [
    "### 7.6 Write a `train` function to combine `train_step` and `test_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDQ-tfoCpxVZ"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# train function\n",
    "def train(model: torch.nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          test_loader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          epochs: int=5,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          device=device):\n",
    "\n",
    "  # Create an empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "             \"train_accuracy\": [],\n",
    "             \"test_loss\": [],\n",
    "             \"test_accuracy\": []}\n",
    "\n",
    "  # Loop through the epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_step(model,\n",
    "                            train_loader,\n",
    "                            loss_fn,\n",
    "                            optimizer,\n",
    "                            device)\n",
    "    test_loss, test_acc = test_step(model,\n",
    "                          test_loader,\n",
    "                          loss_fn,\n",
    "                          device)\n",
    "\n",
    "    # Print out what's happening\n",
    "    print(f\"Epoch: {epoch} | Train_loss: {train_loss:.4f} | Train_acc: {train_acc:.4f} | Test_loss: {test_loss:.4f} | Test_acc: {test_acc:.4f}\")\n",
    "\n",
    "    # Update the results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_accuracy\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_accuracy\"].append(test_acc)\n",
    "\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNUZtNen3r2C"
   },
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
    "                             lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3W5CsJRb4E9M"
   },
   "source": [
    "### 7.7 Training `model_0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236,
     "referenced_widgets": [
      "019c95cf117146a9a0dd5b693023faeb",
      "611ab0a4aea846b1aba45661705ff41b",
      "4aae2db7035e46cb8921533220bce7bb",
      "ba5f97f7ded34ef38123ec4b375697ff",
      "2e59db097dac47779fddf66b839da2af",
      "9e3224b5eec845749151da6f26ebfd61",
      "67e9a3039aff44d7807600a6122d6a51",
      "ca57856fa4614f57b961250f246e8e42",
      "f31692924b754ca89752a5005c4c8507",
      "f630a1bc85a0461ea3df9365ed588001",
      "b4a2aa509afe41b886c142a329857647"
     ]
    },
    "id": "J2X5Xbfc4kiY",
    "outputId": "70f02bb1-0fce-4872-83c9-ac80b3a2071a"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "EPOCHS = 10 # hyperparam\n",
    "\n",
    "# start the timer\n",
    "from timeit import default_timer as Timer\n",
    "start_time = Timer()\n",
    "\n",
    "model_0 = TinyVGG(input_shape=3, hidden_units=10,\n",
    "                  output_shape=len(class_names)).to(device)\n",
    "\n",
    "model_0_results = train(model=model_0,\n",
    "                        train_loader=train_dataloader_simple,\n",
    "                        test_loader=test_dataloader_simple,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=EPOCHS,\n",
    "                        loss_fn=loss_fn,\n",
    "                        device=device)\n",
    "\n",
    "end_time = Timer()\n",
    "total_training_time = end_time - start_time\n",
    "print(f\"Total training time: {total_training_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWXTFkELauyj",
    "outputId": "652fcc84-f8cb-4285-9cb5-1c5a80bdd59b"
   },
   "outputs": [],
   "source": [
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K97cP9va6dY"
   },
   "source": [
    "### 7.8 Plot the loss curves of model_0\n",
    "\n",
    "**Loss curve** tracks a model's progress over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQAgL-o6bhrb",
    "outputId": "f3cc95d0-e90e-49da-d927-d02b1f12699d"
   },
   "outputs": [],
   "source": [
    "model_0_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlhOUcaNbl8U"
   },
   "outputs": [],
   "source": [
    "def plot_loss_curve(results: Dict[str, List[float]]):\n",
    "  \"\"\"Plots training curves for a model results dictionary.\"\"\"\n",
    "  loss = results['train_loss']\n",
    "  test_loss = results['test_loss']\n",
    "  acc = results['train_accuracy']\n",
    "  test_acc = results['test_accuracy']\n",
    "\n",
    "  epochs = range(len(results['train_loss']))\n",
    "\n",
    "  plt.figure(figsize=(14,6))\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.plot(epochs, loss, label='train_loss')\n",
    "  plt.plot(epochs, test_loss, label='test_loss')\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.plot(epochs, acc, label='train_accuracy')\n",
    "  plt.plot(epochs, test_acc, label='test_accuracy')\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "D7uZjgJIeD0x",
    "outputId": "6a062608-7073-48e3-9730-c199d0f3b83f"
   },
   "outputs": [],
   "source": [
    "plot_loss_curve(model_0_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3Dy9OjKh1VE"
   },
   "source": [
    "## 8. Model 1: TinyVGG with Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lVejPV5ilyA"
   },
   "source": [
    "### 8.1 Create transform with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skKCe2GJf9LR"
   },
   "outputs": [],
   "source": [
    "train_transforms_trivial = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms_simple = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9NAlarQlt9U"
   },
   "source": [
    "### 8.2 Create train and test `datasets` and `dataloaders` with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEQajDfLl9cZ",
    "outputId": "b7ed19ae-38e7-453d-d64b-df123dfe53f9"
   },
   "outputs": [],
   "source": [
    "# Create augmented datasets\n",
    "train_data_augmented = datasets.ImageFolder(root=train_dir,\n",
    "                                            transform=train_transforms_trivial,\n",
    "                                            target_transform=None)\n",
    "\n",
    "test_data_simple = datasets.ImageFolder(root=test_dir,\n",
    "                                            transform=test_transforms_simple,\n",
    "                                            target_transform=None)\n",
    "\n",
    "len(train_data_augmented), len(test_data_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rg5NsXmnH5D"
   },
   "outputs": [],
   "source": [
    "# Turn datasets into dataloaders\n",
    "import os\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "torch.manual_seed(42) # we will shuffle the training data here\n",
    "train_dataloader_augmented = DataLoader(dataset=train_data_augmented,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        num_workers=NUM_WORKERS,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_dataloader_new = DataLoader(dataset=test_data_simple,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        num_workers=NUM_WORKERS,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNrBuuPwpnNh"
   },
   "source": [
    "### 8.3 Constructing and training Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUvPMLL7psVc",
    "outputId": "3680ef3b-4ddc-4cbb-e967-bf1799c9fb5b"
   },
   "outputs": [],
   "source": [
    "# Create model 1\n",
    "torch.manual_seed(42)\n",
    "model_1 = TinyVGG(input_shape=3,\n",
    "                  hidden_units=13,\n",
    "                  output_shape=len(class_names)).to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236,
     "referenced_widgets": [
      "b5b0bc8d5e5f4a068b24400b0302b792",
      "a4a82e61eb274d7cac5e84c777e46111",
      "bc80d54fc508403aab027a9479e2d7ab",
      "4307c24d4a704212986f81fc94a5bb2a",
      "2907648c293346b9b415c268cfc18e97",
      "cc2cf841c726482385ccd08bca7767c5",
      "18ce906212284a9bbda2345feda598ad",
      "28b21b689c3f49e3aafa2f5590804890",
      "2d64266dbcfd4eebb09d7d05272dbe2c",
      "c8c09dfc38b24ca7895a30d87835dd40",
      "4ea0be1ee68d4b518321969b6665b448"
     ]
    },
    "id": "8mlS6TKerTZ0",
    "outputId": "fff48caf-c97b-4a58-c392-db55ec6428d9"
   },
   "outputs": [],
   "source": [
    "# Train model 1 on augmented data\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "NUM_EPOCHS = 10 # hyperparam\n",
    "\n",
    "# Setup loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_1.parameters(),\n",
    "                             lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as Timer\n",
    "start_time = Timer()\n",
    "\n",
    "model_1_results = train(model=model_1,\n",
    "                        train_loader=train_dataloader_augmented,\n",
    "                        test_loader=test_dataloader_new,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        loss_fn=loss_fn,\n",
    "                        device=device)\n",
    "\n",
    "end_time = Timer()\n",
    "total_training_time = end_time - start_time\n",
    "print(f\"Total training time: {total_training_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgAvdOUqxVlv"
   },
   "source": [
    "### 8.4 Plot the loss curves of `model_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "I08acLjMsvRK",
    "outputId": "f506d712-efa6-4566-deef-532f2fb57de3"
   },
   "outputs": [],
   "source": [
    "plot_loss_curve(model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJia-LtryVba"
   },
   "source": [
    "## 9. Compare model results\n",
    "\n",
    "some ways to do this:\n",
    "* hard coding\n",
    "* MlFlow\n",
    "* Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZRcnB3N5F5b"
   },
   "source": [
    "### 9.1 Using `Weights and Biases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0vkZio4U32qE",
    "outputId": "6652de46-ded3-49a4-a4fa-eefa3869579e"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 1. Start a new run\n",
    "run_model_0 = wandb.init(project=\"custom_datasets\")\n",
    "# 2. Save model inputs and hyperparameters\n",
    "config_0 = run_model_0.config\n",
    "config_0.dropout = 0.01\n",
    "config_0.num_epochs = NUM_EPOCHS # Log the number of epochs\n",
    "config_0.batch_size = BATCH_SIZE # Log the batch size\n",
    "config_0.learning_rate = optimizer.param_groups[0]['lr'] # Log the learning rate\n",
    "\n",
    "# 3. Log gradients and model parameters (optional, can be resource intensive)\n",
    "# run.watch(model_1) # Keep commented out unless you specifically need to log gradients\n",
    "\n",
    "# The loop below is for demonstrating batch processing, but logging metrics\n",
    "# here without calculating them will cause an error.\n",
    "# Since you are already calculating epoch-level metrics in your 'train' function,\n",
    "# it is more efficient to log those results after training.\n",
    "# Removing the erroneous batch logging loop:\n",
    "# for batch_idx, (data, target) in enumerate(train_dataloader_augmented):\n",
    "#   if batch_idx % log_interval == 0:\n",
    "#    run.log({\"loss\": loss})\n",
    "#    pass\n",
    "\n",
    "# Log the epoch-level results after the training is complete\n",
    "# Ensure model_1_results is available in this scope (it is from the previous cell)\n",
    "if 'model_0_results' in locals():\n",
    "    for epoch in range(len(model_0_results['train_loss'])):\n",
    "        run_model_0.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": model_0_results['train_loss'][epoch],\n",
    "            \"train_accuracy\": model_0_results['train_accuracy'][epoch],\n",
    "            \"test_loss\": model_0_results['test_loss'][epoch],\n",
    "            \"test_accuracy\": model_0_results['test_accuracy'][epoch]\n",
    "        })\n",
    "\n",
    "print(\"Finished evaluating model 0\")\n",
    "\n",
    "# End the W&B run\n",
    "run_model_0.finish()\n",
    "\n",
    "run_model_1 = wandb.init(project=\"custom_datasets\")\n",
    "config_1 = run_model_1.config\n",
    "config_1.dropout = 0.01\n",
    "config_1.num_epochs = NUM_EPOCHS # Log the number of epochs\n",
    "config_1.batch_size = BATCH_SIZE # Log the batch size\n",
    "config_1.learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "if 'model_1_results' in locals():\n",
    "    for epoch in range(len(model_1_results['train_loss'])):\n",
    "        run_model_1.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": model_1_results['train_loss'][epoch],\n",
    "            \"train_accuracy\": model_1_results['train_accuracy'][epoch],\n",
    "            \"test_loss\": model_1_results['test_loss'][epoch],\n",
    "            \"test_accuracy\": model_1_results['test_accuracy'][epoch]\n",
    "        })\n",
    "print(\"Finished evaluating model 1\")\n",
    "\n",
    "run_model_1.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUoWBZkx59Gv"
   },
   "source": [
    "### 9.2 Using hard coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "hxE0Nx9O57O6",
    "outputId": "5c92a453-3d19-47a7-8cb7-90bfc4ade28e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_0_df = pd.DataFrame(model_0_results)\n",
    "model_1_df = pd.DataFrame(model_1_results)\n",
    "model_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "id": "V3eoE9RwfaMZ",
    "outputId": "bd6efa6d-3647-4522-a35f-996a783f4200"
   },
   "outputs": [],
   "source": [
    "# setup a plot\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "# get no of epochs\n",
    "epochs = range(len(model_0_df))\n",
    "\n",
    "# Plot train loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, model_0_df['train_loss'], label='Model_0')\n",
    "plt.plot(epochs, model_1_df['train_loss'], label='Model_1')\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();\n",
    "\n",
    "# Plot test loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, model_0_df['test_loss'], label='Model_0')\n",
    "plt.plot(epochs, model_1_df['test_loss'], label='Model_1')\n",
    "plt.title(\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();\n",
    "\n",
    "# Plot train acc\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, model_0_df['train_accuracy'], label='Model_0')\n",
    "plt.plot(epochs, model_1_df['train_accuracy'], label='Model_1')\n",
    "plt.title(\"Train Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();\n",
    "\n",
    "# plot test acc\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, model_0_df['test_accuracy'], label='Model_0')\n",
    "plt.plot(epochs, model_1_df['test_accuracy'], label='Model_1')\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiN3lWQCicLZ"
   },
   "source": [
    "## 10. Let's make predictions on a custom image now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPNKFpO-BGJk",
    "outputId": "3af1fea0-3f80-42e5-93c2-458e71431da0"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Create path for custom img\n",
    "custom_image_path = data_path / \"Big_pizza.jpeg\"\n",
    "\n",
    "if not custom_image_path.is_file():\n",
    "  with open(custom_image_path, \"wb\") as f:\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/Saloni0512/ImageData/main/Big_pizza.jpeg\")\n",
    "    f.write(request.content)\n",
    "    print(f\"Dowloading {custom_image_path}...\")\n",
    "else:\n",
    "  print(f\"{custom_image_path} already exist, skipping download\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmjgeJo5_AB6"
   },
   "source": [
    "### 10.1 Loading a custom image as tensor\n",
    "* The image must be of size (64,64,3).\n",
    "* The dtype of image tensor must be float32\n",
    "* It must be on the same device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8sNjswlAHyt",
    "outputId": "93581d31-d98a-48d7-dc9a-aedc0fb878e3"
   },
   "outputs": [],
   "source": [
    "custom_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg3fKkod-cFh",
    "outputId": "a5577afd-3a2e-49de-c8a2-6cdbd8d660ba"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Read in image\n",
    "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
    "custom_image_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wWmQHei9eDR",
    "outputId": "44a42a9a-49fd-4837-c4d2-cdb1712fbaf1"
   },
   "outputs": [],
   "source": [
    "custom_image_uint8.shape, custom_image_uint8.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "O_y27AR3ugvY",
    "outputId": "233b82ef-13f6-4dcb-af89-3c86090406e3"
   },
   "outputs": [],
   "source": [
    "plt.imshow(custom_image_uint8.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb106LQM-Niq"
   },
   "source": [
    "### 10.2 Making a prediction on the custom image with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6XioHqL-UMF",
    "outputId": "b7df0b14-f492-4418-eadc-c36d05003cc4"
   },
   "outputs": [],
   "source": [
    "# load in the image and convert it to torch.float32\n",
    "custom_image_tensor = torchvision.io.read_image(str(custom_image_path)).type(torch.float32) / 255\n",
    "custom_image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "0jQRsV0E_vR0",
    "outputId": "cee3396a-c631-4bfc-d755-c9821ed74887"
   },
   "outputs": [],
   "source": [
    "# Create a transform to reduce the shape of image\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "])\n",
    "\n",
    "custom_image_transformed = custom_transform(custom_image_tensor)\n",
    "print(f\"Transformed image shape: {custom_image_transformed.shape}\")\n",
    "plt.imshow(custom_image_transformed.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "ndiJDq0lBWFC",
    "outputId": "46b2481b-e54a-433c-b6ea-f42a2eedee76"
   },
   "outputs": [],
   "source": [
    "# We need to add a batch size here\n",
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "  custom_image_pred = model_1(custom_image_transformed.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhKguGeFCECS",
    "outputId": "33ce62a5-36ef-44d9-b7e1-6a9c18671219"
   },
   "outputs": [],
   "source": [
    "# This should run successfully\n",
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "  custom_image_pred = model_1(custom_image_transformed.unsqueeze(0).to(device))\n",
    "custom_image_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvzidDTbDIYG"
   },
   "source": [
    "> Note: Here, we get raw logits as predictions of the pizza, steak and sushi classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKjH9qeVDhg3",
    "outputId": "9782e57f-1192-4d29-8b81-e6b8d361d28a"
   },
   "outputs": [],
   "source": [
    "# Convert logits into pred probs\n",
    "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
    "custom_image_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x-w6__oD1im",
    "outputId": "7f4662c8-25ac-4ba2-eed4-63ffb3e0c0aa"
   },
   "outputs": [],
   "source": [
    "# Convert pred probs into pred labels\n",
    "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
    "custom_image_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "oi8rtgqgEK5V",
    "outputId": "4e518960-663e-4c47-86b9-b1fc26783515"
   },
   "outputs": [],
   "source": [
    "class_names[custom_image_pred_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp1RtF8TDXNv"
   },
   "source": [
    "### 10.3 Functionising the prediction of custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rS4rYg1tDTXD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNwLx2n7v1CGY+wYAuOLMaY",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "019c95cf117146a9a0dd5b693023faeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_611ab0a4aea846b1aba45661705ff41b",
       "IPY_MODEL_4aae2db7035e46cb8921533220bce7bb",
       "IPY_MODEL_ba5f97f7ded34ef38123ec4b375697ff"
      ],
      "layout": "IPY_MODEL_2e59db097dac47779fddf66b839da2af"
     }
    },
    "18ce906212284a9bbda2345feda598ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28b21b689c3f49e3aafa2f5590804890": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2907648c293346b9b415c268cfc18e97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d64266dbcfd4eebb09d7d05272dbe2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e59db097dac47779fddf66b839da2af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4307c24d4a704212986f81fc94a5bb2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8c09dfc38b24ca7895a30d87835dd40",
      "placeholder": "​",
      "style": "IPY_MODEL_4ea0be1ee68d4b518321969b6665b448",
      "value": " 10/10 [00:10&lt;00:00,  1.00s/it]"
     }
    },
    "4aae2db7035e46cb8921533220bce7bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca57856fa4614f57b961250f246e8e42",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f31692924b754ca89752a5005c4c8507",
      "value": 10
     }
    },
    "4ea0be1ee68d4b518321969b6665b448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "611ab0a4aea846b1aba45661705ff41b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e3224b5eec845749151da6f26ebfd61",
      "placeholder": "​",
      "style": "IPY_MODEL_67e9a3039aff44d7807600a6122d6a51",
      "value": "100%"
     }
    },
    "67e9a3039aff44d7807600a6122d6a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e3224b5eec845749151da6f26ebfd61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4a82e61eb274d7cac5e84c777e46111": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc2cf841c726482385ccd08bca7767c5",
      "placeholder": "​",
      "style": "IPY_MODEL_18ce906212284a9bbda2345feda598ad",
      "value": "100%"
     }
    },
    "b4a2aa509afe41b886c142a329857647": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5b0bc8d5e5f4a068b24400b0302b792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4a82e61eb274d7cac5e84c777e46111",
       "IPY_MODEL_bc80d54fc508403aab027a9479e2d7ab",
       "IPY_MODEL_4307c24d4a704212986f81fc94a5bb2a"
      ],
      "layout": "IPY_MODEL_2907648c293346b9b415c268cfc18e97"
     }
    },
    "ba5f97f7ded34ef38123ec4b375697ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f630a1bc85a0461ea3df9365ed588001",
      "placeholder": "​",
      "style": "IPY_MODEL_b4a2aa509afe41b886c142a329857647",
      "value": " 10/10 [00:13&lt;00:00,  1.29s/it]"
     }
    },
    "bc80d54fc508403aab027a9479e2d7ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28b21b689c3f49e3aafa2f5590804890",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d64266dbcfd4eebb09d7d05272dbe2c",
      "value": 10
     }
    },
    "c8c09dfc38b24ca7895a30d87835dd40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca57856fa4614f57b961250f246e8e42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc2cf841c726482385ccd08bca7767c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f31692924b754ca89752a5005c4c8507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f630a1bc85a0461ea3df9365ed588001": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
